agent: "PPO_Clip"  # choice: PPO_Clip, PPO_KL
env_name: "MiniGrid"
env_id: "MiniGrid-Empty-5x5-v0"
RGBImgPartialObsWrapper: False
ImgObsWrapper: False
vectorize: "Dummy_MiniGrid"
policy: "Categorical_AC"  # choice: Gaussian_AC for continuous actions, Categorical_AC for discrete actions.
representation: "Basic_MLP"
runner: "DRL"

representation_hidden_size: [256,]
actor_hidden_size: [256,]
critic_hidden_size: [256,]
activation: "leaky_relu"

seed: 79811
parallels: 16
running_steps: 100000
horizon_size: 256  # the horizon size for an environment, buffer_size = horizon_size * parallels.
n_epoch: 16
n_minibatch: 8
learning_rate: 0.0001

use_grad_clip: True

vf_coef: 0.25
ent_coef: 0.0
target_kl: 0.001  # for PPO_KL agent
clip_range: 0.2  # for PPO_Clip agent
clip_grad_norm: 0.5
gamma: 0.99
use_gae: True
gae_lambda: 0.95
use_advnorm: True

use_obsnorm: True
use_rewnorm: True
obsnorm_range: 5
rewnorm_range: 5

test_steps: 10000
eval_interval: 1000
test_episode: 5
log_dir: "./logs/ppo/"
model_dir: "./models/ppo/"
