agent: "PPO_Clip"  # choice: PPO_Clip, PPO_KL
env_name: "Drones"
env_id: "HoverAviary"  # choices: ['CtrlAviary', 'HoverAviary', 'VelocityAviary']
obs_type: 'kin'
act_type: 'one_d_rpm'
num_drones: 1
record: False
render: False
obstacles: True
max_episode_steps: 2000
vectorize: "Dummy_Drone"
policy: "Gaussian_AC"  # choice: Gaussian_AC for continuous actions, Categorical_AC for discrete actions.
representation: "Basic_MLP"
runner: "DRL"

representation_hidden_size: [512,]
actor_hidden_size: [512,]
critic_hidden_size: [512,]
activation: "leaky_relu"
activation_action: 'tanh'

seed: 79811
parallels: 10
running_steps: 1000000
horizon_size: 256  # the horizon size for an environment, buffer_size = horizon_size * parallels.
n_epoch: 16
n_minibatch: 8
learning_rate: 0.0004

use_grad_clip: True

vf_coef: 0.25
ent_coef: 0.0
target_kl: 0.001  # for PPO_KL agent
clip_range: 0.2  # for PPO_Clip agent
clip_grad_norm: 0.5
gamma: 0.99
use_gae: True
gae_lambda: 0.95
use_advnorm: True

use_obsnorm: True
use_rewnorm: True
obsnorm_range: 5
rewnorm_range: 5

test_steps: 10000
eval_interval: 5000
test_episode: 5
log_dir: "./logs/ppo/"
model_dir: "./models/ppo/"
