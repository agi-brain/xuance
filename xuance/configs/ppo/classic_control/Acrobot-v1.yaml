agent: "PPO_Clip"  # Choice: PPO_Clip, PPO_KL
env_name: "Classic Control"
env_id: "Acrobot-v1"
vectorize: "Dummy_Gym"
representation: "Basic_MLP"
policy: "Categorical_AC"
runner: "DRL"

representation_hidden_size: [128,]
actor_hidden_size: [128,]
critic_hidden_size: [128,]
activation: 'leaky_relu'

seed: 1
parallels: 10
running_steps: 300000
horizon_size: 256  # the horizon size for an environment, buffer_size = horizon_size * parallels.
n_epoch: 8
n_minibatch: 8
learning_rate: 0.0004

use_grad_clip: True

vf_coef: 0.25
ent_coef: 0.01
target_kl: 0.001  # for PPO_KL agent
clip_range: 0.2  # for PPO_Clip agent
clip_grad_norm: 0.5
gamma: 0.98
use_gae: True
gae_lambda: 0.95
use_advnorm: True

use_obsnorm: True
use_rewnorm: True
obsnorm_range: 5
rewnorm_range: 5

test_steps: 10000
eval_interval: 50000
test_episode: 3
log_dir: "./logs/ppo/"
model_dir: "./models/ppo/"
