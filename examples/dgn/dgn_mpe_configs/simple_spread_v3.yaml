dl_toolbox: "torch"  # The deep learning toolbox. Choices: "torch", "mindspore", "tensorlayer"
project_name: "XuanCe_Benchmark"
logger: "tensorboard"  # Choices: tensorboard, wandb.
wandb_user_name: "your_user_name"
render: True
render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.
fps: 15
test_mode: False
device: "cuda:0"  # Choose an calculating device. PyTorch: "cpu", "cuda:0"; TensorFlow: "cpu"/"CPU", "gpu"/"GPU"; MindSpore: "CPU", "GPU", "Ascend", "Davinci".
distributed_training: False  # Whether to use multi-GPU for distributed training.
master_port: '12355'  # The master port for current experiment when use distributed training.

agent: "DGN"  # The agent name.
env_name: "mpe"  # The environment name.
env_id: "simple_spread_v3"  # The environment id.
env_seed: 1
continuous_action: False  # If to use continuous control.
learner: "DGN_Learner"
policy: "DGN_Policy"  # The policy name.
representation: "Basic_RNN"  # The representation name.
vectorize: "SubprocVecMultiAgentEnv"  # The method to vectorize your environment such that can run in parallel.

# recurrent settings for Basic_RNN representation
use_rnn: True  # Whether to use recurrent neural networks.
rnn: "GRU"  # Choice of recurrent networks: GRU or LSTM.
N_recurrent_layers: 1  # Number of recurrent layers.
fc_hidden_sizes: [128, 128, 128]
recurrent_hidden_size: 128
dropout: 0  # dropout should be a number in range [0, 1], the probability of an element being zeroed.

representation_hidden_size: [128, ]
q_hidden_size: [128, ]  # the units for each hidden layer
activation: "sigmoid"  # The activation function of each hidden layer.
use_parameter_sharing: True  # If to use parameter sharing for all agents' policies.
use_actions_mask: False  # If to use actions mask for unavailable actions.

seed: 5
parallels: 16
buffer_size: 10000
batch_size: 32
learning_rate: 0.0007
gamma: 0.95  # discount factor
double_q: True  # use double q learning

start_greedy: 1.0
end_greedy: 0.1
decay_step_greedy: 2000000
start_training: 1000  # start training after n steps
running_steps: 10000000
n_epochs: 8  # The number of training epochs after interaction.
sync_frequency: 200
attention_head: 2
convolution_layer: 2

use_grad_clip: False
grad_clip_norm: 0.5

eval_interval: 100000
test_episode: 5

log_dir: "./logs/dgn/"
model_dir: "./models/dgn/"
