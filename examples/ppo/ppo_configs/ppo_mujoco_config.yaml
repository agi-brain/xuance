dl_toolbox: "torch"  # The deep learning toolbox. Choices: "torch", "mindspore", "tensorlayer"
project_name: "XuanCe_Benchmark"
logger: "tensorboard"  # Choices: tensorboard, wandb.
wandb_user_name: "your_user_name"  # The username of wandb when the logger is wandb.
render: False # Whether to render the environment when testing.
render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.
fps: 50  # The frames per second for the rendering videos in log file.
test_mode: False  # Whether to run in test mode.
device: "cuda:0"  # Choose an calculating device.

agent: "PPO_Clip"  # The agent name.
env_name: "MuJoCo"  # The environment device.
env_id: "Ant-v4"  # The environment id.
vectorize: "DummyVecEnv"  # The vecrized method to create n parallel environments. Choices: DummyVecEnv, or SubprocVecEnv.
learner: "PPOCLIP_Learner"
policy: "Gaussian_AC"  # choice: Gaussian_AC for continuous actions, Categorical_AC for discrete actions.
representation: "Basic_MLP"  # The representation name.

representation_hidden_size: [256,]  # The size of hidden layers for representation network.
actor_hidden_size: [256,]  # The size of hidden layers for actor network.
critic_hidden_size: [256,]  # The size of hidden layers for critic network.
activation: "leaky_relu"  # The activation function for each hidden layer.
activation_action: 'tanh'  # The activation function for the last layer of actor network.

seed: 79811  # The random seed.
parallels: 16  # The number of environments to run in parallel.
running_steps: 1000000  # The total running steps for all environments.
horizon_size: 256  # the horizon size for an environment, buffer_size = horizon_size * parallels.
n_epochs: 16  # The number of training epochs.
n_minibatch: 8  # The number of minibatch for each training epoch. batch_size = buffer_size // n_minibatch.
learning_rate: 0.0004  # The learning rate.

vf_coef: 0.25  # Coefficient factor for critic loss.
ent_coef: 0.0  # Coefficient factor for entropy loss.
target_kl: 0.25  # For PPO_KL learner.
kl_coef: 1.0  # For PPO_KL learner.
clip_range: 0.2  # The clip range for ratio in PPO_Clip learner.
gamma: 0.99  # Discount factor.
use_gae: True  # Use GAE trick.
gae_lambda: 0.95  # The GAE lambda.
use_advnorm: True  # Whether to use advantage normalization.

use_grad_clip: True  # Whether to clip the gradient during training.
clip_type: 1  # Gradient clip for Mindspore: 0: ms.ops.clip_by_value; 1: ms.nn.ClipByNorm()
grad_clip_norm: 0.5  # The max norm of the gradient.
use_actions_mask: False  # Whether to use action mask values.
use_obsnorm: True  # Whether to use observation normalization.
use_rewnorm: True  # Whether to use reward normalization.
obsnorm_range: 5  # The range of observation if use observation normalization.
rewnorm_range: 5  # The range of reward if use reward normalization.

test_steps: 10000  # The total steps for testing.
eval_interval: 5000  # The evaluate interval when use benchmark method.
test_episode: 5  # The test episodes.
log_dir: "./logs/ppo/"  # The main directory of log files.
model_dir: "./models/ppo/"  # The main directory of model files.
