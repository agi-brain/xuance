{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Multi-Agent Environment\n",
    "\n",
    "In XuanCe, users also have the flexibility to create and run their own customized environments with multiple agents in addition to utilizing the provided ones.\n",
    "\n",
    "We need to install XuanCe before getting started.\n",
    "\n",
    "(Note: --quiet is optional and only suppresses output in Google Colab; it's not required for installing XuanCe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1372d0f2ea4aa52b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install xuance --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fc1463ae9eba839"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a new multi-agent environment\n",
    "\n",
    "First, you need to prepare an original environment, i.e., an Partial Observed Markov decision process (POMDP).\n",
    "Then define a new environment based on the basic class ``RawMultiAgentEnv`` of XuanCe.\n",
    "\n",
    "Here is an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f50e315e4e3dea19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium.spaces import Box\n",
    "from xuance.environment import RawMultiAgentEnv, REGISTRY_MULTI_AGENT_ENV\n",
    "\n",
    "class MyNewMultiAgentEnv(RawMultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        super(MyNewMultiAgentEnv, self).__init__()\n",
    "        self.env_id = env_config.env_id\n",
    "        self.num_agents = 3\n",
    "        self.agents = [f\"agent_{i}\" for i in range(self.num_agents)]\n",
    "        self.state_space = Box(-np.inf, np.inf, shape=[8, ])\n",
    "        self.observation_space = {agent: Box(-np.inf, np.inf, shape=[8, ]) for agent in self.agents}\n",
    "        self.action_space = {agent: Box(-np.inf, np.inf, shape=[2, ]) for agent in self.agents}\n",
    "        self.max_episode_steps = 25\n",
    "        self._current_step = 0\n",
    "\n",
    "    def get_env_info(self):\n",
    "        return {'state_space': self.state_space,\n",
    "                'observation_space': self.observation_space,\n",
    "                'action_space': self.action_space,\n",
    "                'agents': self.agents,\n",
    "                'num_agents': self.num_agents,\n",
    "                'max_episode_steps': self.max_episode_steps}\n",
    "\n",
    "    def avail_actions(self):\n",
    "        return None\n",
    "\n",
    "    def agent_mask(self):\n",
    "        \"\"\"Returns boolean mask variables indicating which agents are currently alive.\"\"\"\n",
    "        return {agent: True for agent in self.agents}\n",
    "\n",
    "    def state(self):\n",
    "        \"\"\"Returns the global state of the environment.\"\"\"\n",
    "        return self.state_space.sample()\n",
    "\n",
    "    def reset(self):\n",
    "        observation = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "        info = {}\n",
    "        self._current_step = 0\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self._current_step += 1\n",
    "        observation = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "        rewards = {agent: np.random.random() for agent in self.agents}\n",
    "        terminated = {agent: False for agent in self.agents}\n",
    "        truncated = False if self._current_step < self.max_episode_steps else True\n",
    "        info = {}\n",
    "        return observation, rewards, terminated, truncated, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return np.ones([64, 64, 64])\n",
    "\n",
    "    def close(self):\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eadfe9ae98fdd393"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a config file\n",
    "\n",
    "Then, you need to create a YAML file by following the step 1 in :doc:`Further Usage <further_usage>`.\n",
    "\n",
    "Here is an example of configurations for DDPG algorithm, named \"ippo_new_configs.yaml\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98c8906d6949e8a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "yaml_content = textwrap.dedent(\"\"\"\n",
    "    dl_toolbox: \"torch\"  # The deep learning toolbox. Choices: \"torch\", \"mindspore\", \"tensorlayer\"\n",
    "    project_name: \"XuanCe_Benchmark\"\n",
    "    logger: \"tensorboard\"  # Choices: tensorboard, wandb.\n",
    "    wandb_user_name: \"your_user_name\"\n",
    "    render: True\n",
    "    render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.\n",
    "    fps: 15\n",
    "    test_mode: False\n",
    "    device: \"cpu\"  # Choose an calculating device. PyTorch: \"cpu\", \"cuda:0\"; TensorFlow: \"cpu\"/\"CPU\", \"gpu\"/\"GPU\"; MindSpore: \"CPU\", \"GPU\", \"Ascend\", \"Davinci\".\n",
    "    distributed_training: False  # Whether to use multi-GPU for distributed training.\n",
    "    master_port: '12355'  # The master port for current experiment when use distributed training.\n",
    "\n",
    "    agent: \"IPPO\"\n",
    "    env_name: \"MyNewMultiAgentEnv\"\n",
    "    env_id: \"new_env_id\"\n",
    "    env_seed: 1\n",
    "    continuous_action: True  # Continuous action space or not.\n",
    "    learner: \"IPPO_Learner\"  # The learner name.\n",
    "    policy: \"Gaussian_MAAC_Policy\"\n",
    "    representation: \"Basic_MLP\"\n",
    "    vectorize: \"DummyVecMultiAgentEnv\"\n",
    "\n",
    "    # recurrent settings for Basic_RNN representation.\n",
    "    use_rnn: False  # If to use recurrent neural network as representation. (The representation should be \"Basic_RNN\").\n",
    "    rnn: \"GRU\"  # The type of recurrent layer.\n",
    "    fc_hidden_sizes: [64, 64, 64]  # The hidden size of feed forward layer in RNN representation.\n",
    "    recurrent_hidden_size: 64  # The hidden size of the recurrent layer.\n",
    "    N_recurrent_layers: 1  # The number of recurrent layer.\n",
    "    dropout: 0  # dropout should be a number in range [0, 1], the probability of an element being zeroed.\n",
    "    normalize: \"LayerNorm\"  # Layer normalization.\n",
    "    initialize: \"orthogonal\"  # Network initializer.\n",
    "    gain: 0.01  # Gain value for network initialization.\n",
    "\n",
    "    # recurrent settings for Basic_RNN representation.\n",
    "    representation_hidden_size: [64, ]  # A list of hidden units for each layer of Basic_MLP representation networks.\n",
    "    actor_hidden_size: [64, ]  # A list of hidden units for each layer of actor network.\n",
    "    critic_hidden_size: [64, ]  # A list of hidden units for each layer of critic network.\n",
    "    activation: \"relu\"  # The activation function of each hidden layer.\n",
    "    activation_action: \"sigmoid\"  # The activation function for the last layer of the actor.\n",
    "    use_parameter_sharing: True  # If to use parameter sharing for all agents' policies.\n",
    "    use_actions_mask: False  # If to use actions mask for unavailable actions.\n",
    "\n",
    "    seed: 1  # Random seed.\n",
    "    parallels: 16  # The number of environments to run in parallel.\n",
    "    buffer_size: 3200  # Number of the transitions (use_rnn is False), or the episodes (use_rnn is True) in replay buffer.\n",
    "    n_epochs: 10  # Number of epochs to train.\n",
    "    n_minibatch: 1 # Number of minibatch to sample and train.  batch_size = buffer_size // n_minibatch.\n",
    "    learning_rate: 0.0007  # Learning rate.\n",
    "    weight_decay: 0  # The steps to decay the greedy epsilon.\n",
    "\n",
    "    vf_coef: 0.5  # Coefficient factor for critic loss.\n",
    "    ent_coef: 0.01  # Coefficient factor for entropy loss.\n",
    "    target_kl: 0.25  # For MAPPO_KL learner.\n",
    "    clip_range: 0.2  # The clip range for ratio in MAPPO_Clip learner.\n",
    "    gamma: 0.99  # Discount factor.\n",
    "\n",
    "    # tricks\n",
    "    use_linear_lr_decay: False  # If to use linear learning rate decay.\n",
    "    end_factor_lr_decay: 0.5  # The end factor for learning rate scheduler.\n",
    "    use_global_state: False  # If to use global state to replace merged observations.\n",
    "    use_value_clip: True  # Limit the value range.\n",
    "    value_clip_range: 0.2  # The value clip range.\n",
    "    use_value_norm: True  # Use running mean and std to normalize rewards.\n",
    "    use_huber_loss: True  # True: use huber loss; False: use MSE loss.\n",
    "    huber_delta: 10.0  # The threshold at which to change between delta-scaled L1 and L2 loss. (For huber loss).\n",
    "    use_advnorm: True  # If to use advantage normalization.\n",
    "    use_gae: True  # Use GAE trick.\n",
    "    gae_lambda: 0.95  # The GAE lambda.\n",
    "    use_grad_clip: True  # Gradient normalization.\n",
    "    grad_clip_norm: 10.0  # The max norm of the gradient.\n",
    "    clip_type: 1  # Gradient clip for Mindspore: 0: ms.ops.clip_by_value; 1: ms.nn.ClipByNorm().\n",
    "\n",
    "    running_steps: 100000  # The total running steps.\n",
    "    eval_interval: 10000  # The interval between every two trainings.\n",
    "    test_episode: 5  # The episodes to test in each test period.\n",
    "\n",
    "    log_dir: \"./logs/ippo/\"\n",
    "    model_dir: \"./models/ippo/\"\n",
    "\"\"\")\n",
    "\n",
    "with open(\"ippo_new_configs.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac6fa058118235c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run your multi-agent environment in XuanCe\n",
    "\n",
    "Here is the example of IPPO algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff6c4fd0472684e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.agents import IPPO_Agents\n",
    "\n",
    "configs_dict = get_configs(file_dir=\"ippo_new_configs.yaml\")\n",
    "configs = argparse.Namespace(**configs_dict)\n",
    "REGISTRY_MULTI_AGENT_ENV[configs.env_name] = MyNewMultiAgentEnv  # Register your environment. (Required)\n",
    "\n",
    "envs = make_envs(configs)  # Make parallel environments.\n",
    "Agent = IPPO_Agents(config=configs, envs=envs)  # Create a DDPG agent from XuanCe.\n",
    "Agent.train(configs.running_steps // configs.parallels)  # Train the model for numerous steps.\n",
    "Agent.save_model(\"final_train_model.pth\")  # Save the model to model_dir.\n",
    "Agent.finish()  # Finish the training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8da7ee9cadffa173"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test your model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e384ba48c0750a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.agents import IPPO_Agents\n",
    "\n",
    "configs_dict = get_configs(file_dir=\"ippo_new_configs.yaml\")\n",
    "configs = argparse.Namespace(**configs_dict)\n",
    "REGISTRY_MULTI_AGENT_ENV[configs.env_name] = MyNewMultiAgentEnv  # Register your environment. (Required)\n",
    "configs.parallels = 1\n",
    "\n",
    "envs_fn = lambda: make_envs(configs)  # The environment function for testing.\n",
    "Agent = IPPO_Agents(config=configs, envs=envs_fn())  # Create a DDPG agent from XuanCe.\n",
    "Agent.load_model(configs.model_dir)  # Load the pre-trained model.\n",
    "scores = Agent.test(envs_fn, configs.test_episode)  # Test the model.\n",
    "\n",
    "print(\"Test episode returns: \")\n",
    "for i, score in enumerate(scores):\n",
    "  print(f\"Episode {i}: {scores[i]}\")\n",
    "print(f\"Average returns: {sum(scores)/len(scores)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d90a78260fa5627"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full code\n",
    "\n",
    "The full code for the above steps can be visited in this link: [https://github.com/agi-brain/xuance/blob/master/examples/new_environments/ippo_new_env.py](https://github.com/agi-brain/xuance/blob/master/examples/new_environments/ippo_new_env.py)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50cf25a3d1ee12b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
