{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Further Usage\n",
    "\n",
    "The previous page demonstrated how to directly run an algorithm by calling the runner.\n",
    "In order to help users better understand the internal implementation process of \"XuanCe\",\n",
    "and facilitate further algorithm development and implementation of their own reinforcement learning tasks,\n",
    "this section will take the PPO algorithm training on the MuJoCo environment task as an example,\n",
    "and provide a detailed introduction on how to call the API from the bottom level to implement reinforcement learning model training.\n",
    "\n",
    "To get started, install XuanCe first.\n",
    "\n",
    "(Note: --quiet is optional and only suppresses output in Google Colab; it's not required for installing XuanCe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2df4c7442e37c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install xuance --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfd65b19a58037e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the config file\n",
    "\n",
    "A config file should contain the necessary arguments of a PPO agent, and should be a YAML file.\n",
    "Here we show a config file named \"ppo_mujoco_config.yaml\" for MuJoCo environment in gym.\n",
    "You can also create this file by running the following code directly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89cef5168e573fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "yaml_content = textwrap.dedent(\"\"\"\n",
    "    dl_toolbox: \"torch\"  # The deep learning toolbox. Choices: \"torch\", \"mindspore\", \"tensorlayer\"\n",
    "    project_name: \"XuanCe_Benchmark\"\n",
    "    logger: \"tensorboard\"  # Choices: tensorboard, wandb.\n",
    "    wandb_user_name: \"your_user_name\"  # The username of wandb when the logger is wandb.\n",
    "    render: False # Whether to render the environment when testing.\n",
    "    render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.\n",
    "    fps: 50  # The frames per second for the rendering videos in log file.\n",
    "    test_mode: False  # Whether to run in test mode.\n",
    "    device: \"cpu\"  # Choose an calculating device.\n",
    "    distributed_training: False  # Whether to use multi-GPU for distributed training.\n",
    "    master_port: '12355'  # The master port for current experiment when use distributed training.\n",
    "    \n",
    "    agent: \"PPO_Clip\"  # The agent name.\n",
    "    env_name: \"Classic Control\"  # The environment device.\n",
    "    env_id: \"Pendulum-v1\"  # The environment id.\n",
    "    env_seed: 1  # Random seed for environment.\n",
    "    vectorize: \"DummyVecEnv\"  # The vecrized method to create n parallel environments. Choices: DummyVecEnv, or SubprocVecEnv.\n",
    "    learner: \"PPOCLIP_Learner\"  # The learner.\n",
    "    policy: \"Gaussian_AC\"  # choice: Gaussian_AC for continuous actions, Categorical_AC for discrete actions.\n",
    "    representation: \"Basic_MLP\"  # The representation name.\n",
    "    \n",
    "    representation_hidden_size: [128,]  # The size of hidden layers for representation network.\n",
    "    actor_hidden_size: [128,]  # The size of hidden layers for actor network.\n",
    "    critic_hidden_size: [128,]  # The size of hidden layers for critic network.\n",
    "    activation: \"leaky_relu\"  # The activation function for each hidden layer.\n",
    "    activation_action: 'tanh'  # The activation function for the last layer of actor network.\n",
    "    \n",
    "    seed: 1  # The random seed.\n",
    "    parallels: 10  # The number of environments to run in parallel.\n",
    "    running_steps: 300000  # The total running steps for all environments.\n",
    "    horizon_size: 256  # the horizon size for an environment, buffer_size = horizon_size * parallels.\n",
    "    n_epochs: 8  # The number of training epochs.\n",
    "    n_minibatch: 8  # The number of minibatch for each training epoch. batch_size = buffer_size // n_minibatch.\n",
    "    learning_rate: 0.0004  # The learning rate.\n",
    "    \n",
    "    vf_coef: 0.25  # Coefficient factor for critic loss.\n",
    "    ent_coef: 0.01  # Coefficient factor for entropy loss.\n",
    "    target_kl: 0.25  # For PPO_KL learner.\n",
    "    kl_coef: 1.0  # For PPO_KL learner.\n",
    "    clip_range: 0.2  # The clip range for ratio in PPO_Clip learner.\n",
    "    gamma: 0.98  # Discount factor.\n",
    "    use_gae: True  # Use GAE trick.\n",
    "    gae_lambda: 0.95  # The GAE lambda.\n",
    "    use_advnorm: True  # Whether to use advantage normalization.\n",
    "    \n",
    "    use_grad_clip: True  # Whether to clip the gradient during training.\n",
    "    clip_type: 1  # Gradient clip for Mindspore: 0: ms.ops.clip_by_value; 1: ms.nn.ClipByNorm()\n",
    "    grad_clip_norm: 0.5  # The max norm of the gradient.\n",
    "    use_actions_mask: False  # Whether to use action mask values.\n",
    "    use_obsnorm: True  # Whether to use observation normalization.\n",
    "    use_rewnorm: True  # Whether to use reward normalization.\n",
    "    obsnorm_range: 5  # The range of observation if use observation normalization.\n",
    "    rewnorm_range: 5  # The range of reward if use reward normalization.\n",
    "    \n",
    "    test_steps: 10000  # The total steps for testing.\n",
    "    eval_interval: 50000  # The evaluate interval when use benchmark method.\n",
    "    test_episode: 5  # The test episodes.\n",
    "    log_dir: \"./logs/ppo/\"  # The main directory of log files.\n",
    "    model_dir: \"./models/ppo/\"  # The main directory of model files.\n",
    "\"\"\")\n",
    "\n",
    "with open(\"ppo_pendulum_config.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f027bb05bf0f33f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run an example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eee3f696530d832"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.utils.operations import set_seed\n",
    "from xuance.torch.agents import PPOCLIP_Agent\n",
    "\n",
    "configs_dict = get_configs(file_dir=\"ppo_pendulum_config.yaml\")\n",
    "configs = argparse.Namespace(**configs_dict)\n",
    "\n",
    "set_seed(configs.seed)\n",
    "envs = make_envs(configs)\n",
    "Agent = PPOCLIP_Agent(config=configs, envs=envs)\n",
    "\n",
    "train_information = {\"Deep learning toolbox\": configs.dl_toolbox,\n",
    "                     \"Calculating device\": configs.device,\n",
    "                     \"Algorithm\": configs.agent,\n",
    "                     \"Environment\": configs.env_name,\n",
    "                     \"Scenario\": configs.env_id}\n",
    "for k, v in train_information.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "def env_fn():\n",
    "    configs_test = deepcopy(configs)\n",
    "    configs_test.parallels = configs_test.test_episode\n",
    "    return make_envs(configs_test)\n",
    "\n",
    "train_steps = configs.running_steps // configs.parallels\n",
    "eval_interval = configs.eval_interval // configs.parallels\n",
    "test_episode = configs.test_episode\n",
    "num_epoch = int(train_steps / eval_interval)\n",
    "\n",
    "test_scores = Agent.test(env_fn, test_episode)\n",
    "Agent.save_model(model_name=\"best_model.pth\")\n",
    "best_scores_info = {\"mean\": np.mean(test_scores),\n",
    "                    \"std\": np.std(test_scores),\n",
    "                    \"step\": Agent.current_step}\n",
    "for i_epoch in range(num_epoch):\n",
    "    print(\"Epoch: %d/%d:\" % (i_epoch, num_epoch))\n",
    "    Agent.train(eval_interval)\n",
    "    test_scores = Agent.test(env_fn, test_episode)\n",
    "\n",
    "    if np.mean(test_scores) > best_scores_info[\"mean\"]:\n",
    "        best_scores_info = {\"mean\": np.mean(test_scores),\n",
    "                            \"std\": np.std(test_scores),\n",
    "                            \"step\": Agent.current_step}\n",
    "        # save best model\n",
    "        Agent.save_model(model_name=\"best_model.pth\")\n",
    "# end benchmarking\n",
    "print(\"Best Model Score: %.2f, std=%.2f\" % (best_scores_info[\"mean\"], best_scores_info[\"std\"]))\n",
    "Agent.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6e0f4e8b6139b19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
