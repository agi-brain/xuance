{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Single-Agent Environment\n",
    "\n",
    "In XuanCe, users have the flexibility to create and run their own customized environments in addition to utilizing the provided ones.\n",
    "\n",
    "We need to install XuanCe before getting started.\n",
    "\n",
    "(Note: --quiet is optional and only suppresses output in Google Colab; it's not required for installing XuanCe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2606aae726b1f3fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install xuance --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "650fbb0d1fcc1984"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a new environment\n",
    "\n",
    "First, you need to prepare an original environment, i.e., an Markov decision process.\n",
    "Then define a new environment based on the basic class ``RawEnvironment`` of XuanCe.\n",
    "After defining a new class of environment, you need to add it to the ``REGISTRY_ENV``.\n",
    "\n",
    "Here is an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44421108ac42dda0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium.spaces import Box\n",
    "from xuance.environment import RawEnvironment, REGISTRY_ENV\n",
    "\n",
    "class MyNewEnv(RawEnvironment):\n",
    "    def __init__(self, env_config):\n",
    "        super(MyNewEnv, self).__init__()\n",
    "        self.env_id = env_config.env_id  # The environment id.\n",
    "        self.observation_space = Box(-np.inf, np.inf, shape=[18, ])  # Define observation space.\n",
    "        self.action_space = Box(-np.inf, np.inf, shape=[5, ])  # Define action space. In this example, the action space is continuous.\n",
    "        self.max_episode_steps = 32  # The max episode length.\n",
    "        self._current_step = 0  # The count of steps of current episode.\n",
    "\n",
    "    def reset(self, **kwargs):  # Reset your environment.\n",
    "        self._current_step = 0\n",
    "        return self.observation_space.sample(), {}\n",
    "\n",
    "    def step(self, action):  # Run a step with an action.\n",
    "        self._current_step += 1\n",
    "        observation = self.observation_space.sample()\n",
    "        rewards = np.random.random()\n",
    "        terminated = False\n",
    "        truncated = False if self._current_step < self.max_episode_steps else True\n",
    "        info = {}\n",
    "        return observation, rewards, terminated, truncated, info\n",
    "\n",
    "    def render(self, *args, **kwargs):  # Render your environment and return an image if the render_mode is \"rgb_array\".\n",
    "        return np.ones([64, 64, 64])\n",
    "\n",
    "    def close(self):  # Close your environment.\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8618bfd3ce71b0d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a config file\n",
    "\n",
    "Then, you need to create a YAML file by following the step 1 in :doc:`Further Usage <further_usage>`.\n",
    "\n",
    "Here is an example of configurations for DDPG algorithm, named \"ddpg_new_env.yaml\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab8e4780de2b1cb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "yaml_content = textwrap.dedent(\"\"\"\n",
    "    dl_toolbox: \"torch\"  # The deep learning toolbox. Choices: \"torch\", \"mindspore\", \"tensorlayer\"\n",
    "    project_name: \"XuanCe_Benchmark\"\n",
    "    logger: \"tensorboard\"  # Choices: tensorboard, wandb.\n",
    "    wandb_user_name: \"your_user_name\"\n",
    "    render: True\n",
    "    render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.\n",
    "    fps: 50\n",
    "    test_mode: False\n",
    "    device: \"cpu\"\n",
    "    distributed_training: False\n",
    "    master_port: '12355'\n",
    "\n",
    "    agent: \"DDPG\"\n",
    "    env_name: \"MyNewEnv\"\n",
    "    env_id: \"new-v1\"\n",
    "    env_seed: 1\n",
    "    vectorize: \"DummyVecEnv\"\n",
    "    policy: \"DDPG_Policy\"\n",
    "    representation: \"Basic_Identical\"\n",
    "    learner: \"DDPG_Learner\"\n",
    "    runner: \"DRL\"\n",
    "\n",
    "    representation_hidden_size:  # If you choose Basic_Identical representation, then ignore this value\n",
    "    actor_hidden_size: [400, 300]\n",
    "    critic_hidden_size: [400, 300]\n",
    "    activation: \"leaky_relu\"\n",
    "    activation_action: 'tanh'\n",
    "\n",
    "    seed: 19089\n",
    "    parallels: 4  # number of environments\n",
    "    buffer_size: 200000  # replay buffer size\n",
    "    batch_size: 100\n",
    "    learning_rate_actor: 0.001\n",
    "    learning_rate_critic: 0.001\n",
    "    gamma: 0.99\n",
    "    tau: 0.005\n",
    "\n",
    "    start_noise: 0.5\n",
    "    end_noise: 0.1\n",
    "    training_frequency: 1\n",
    "    running_steps: 10000\n",
    "    start_training: 1000\n",
    "\n",
    "    use_grad_clip: False  # gradient normalization\n",
    "    grad_clip_norm: 0.5\n",
    "    use_obsnorm: False\n",
    "    use_rewnorm: False\n",
    "    obsnorm_range: 5\n",
    "    rewnorm_range: 5\n",
    "\n",
    "    test_steps: 10000\n",
    "    eval_interval: 5000\n",
    "    test_episode: 5\n",
    "\n",
    "    log_dir: \"./logs/ddpg/\"\n",
    "    model_dir: \"./models/ddpg/\"\n",
    "\"\"\")\n",
    "\n",
    "with open(\"ddpg_new_env.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb4a6fdbc51ed8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run your environment in XuanCe\n",
    "\n",
    "Here is an example of DDPG algorithm for the customized environment:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18bdc1ae4061ce6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.agents import DDPG_Agent\n",
    "\n",
    "configs_dict = get_configs(file_dir=\"ddpg_new_env.yaml\")\n",
    "configs = argparse.Namespace(**configs_dict)\n",
    "REGISTRY_ENV[configs.env_name] = MyNewEnv  # Register your environment. (Required)\n",
    "\n",
    "envs = make_envs(configs)  # Make parallel environments.\n",
    "Agent = DDPG_Agent(config=configs, envs=envs)  # Create a DDPG agent from XuanCe.\n",
    "Agent.train(configs.running_steps // configs.parallels)  # Train the model for numerous steps.\n",
    "Agent.save_model(\"final_train_model.pth\")  # Save the model to model_dir.\n",
    "Agent.finish()  # Finish the training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc599e6248b9ac28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test your model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d7014df7a74bf6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.agents import DDPG_Agent\n",
    "\n",
    "configs_dict = get_configs(file_dir=\"ddpg_new_env.yaml\")\n",
    "configs = argparse.Namespace(**configs_dict)\n",
    "REGISTRY_ENV[configs.env_name] = MyNewEnv  # Register your environment. (Required)\n",
    "configs.parallels = 1\n",
    "\n",
    "envs_fn = lambda: make_envs(configs)  # The environment function for testing.\n",
    "Agent = DDPG_Agent(config=configs, envs=envs_fn())  # Create a DDPG agent from XuanCe.\n",
    "Agent.load_model(configs.model_dir)  # Load the pre-trained model.\n",
    "scores = Agent.test(envs_fn, configs.test_episode)  # Test the model.\n",
    "\n",
    "print(\"Test episode returns: \")\n",
    "for i, score in enumerate(scores):\n",
    "  print(f\"Episode {i}: {scores[i]}\")\n",
    "print(f\"Average returns: {sum(scores)/len(scores)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60944b934a951363"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full code\n",
    "\n",
    "The full code for the above steps can be visited in this link: \n",
    "[https://github.com/agi-brain/xuance/blob/master/examples/new_environments/ddpg_new_env.py](https://github.com/agi-brain/xuance/blob/master/examples/new_environments/ddpg_new_env.py)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c728bd9e56784ded"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
