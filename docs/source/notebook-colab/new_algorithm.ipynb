{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# New Algorithm\n",
    "\n",
    "We allow users create their own custom algorithm outside of the default in XuanCe.\n",
    "\n",
    "This tutorial walks you through the process of creating, training,\n",
    "and testing a custom off-policy reinforcement learning (RL) agent using the XuanCe framework.\n",
    "The demo involves defining a custom policy, learner, and agent while using XuanCe’s modular architecture for RL experiments.\n",
    "\n",
    "To get started, install XuanCe first.\n",
    "\n",
    "(Note: --quiet is optional and only suppresses output in Google Colab; it's not required for installing XuanCe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9fd3165e9e64335"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install xuance --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb7da667cd63d64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the modules of Policy, Learner, and Agent \n",
    "\n",
    "The policy is the brain of the agent.\n",
    "It maps observations to actions, optionally through a value function.\n",
    "\n",
    "The learner manages the policy optimization process,\n",
    "including computing loss, performing gradient updates, and synchronizing target networks.\n",
    "\n",
    "The agent combines the policy, learner, and environment interaction to create a complete RL pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6617823f8184e7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from argparse import Namespace\n",
    "from xuance.common import get_configs\n",
    "from xuance.environment import make_envs\n",
    "from xuance.torch.agents import OffPolicyAgent\n",
    "from xuance.torch.learners import Learner, REGISTRY_Learners\n",
    "\n",
    "class MyPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    An example of self-defined policy.\n",
    "\n",
    "    Args:\n",
    "        representation (nn.Module): A neural network module responsible for extracting meaningful features from the raw observations provided by the environment.\n",
    "        hidden_dim (int): Specifies the number of units in each hidden layer, determining the model’s capacity to capture complex patterns.\n",
    "        n_actions (int): The total number of discrete actions available to the agent in the environment.\n",
    "        device (torch.device): The calculating device.\n",
    "\n",
    "\n",
    "    Note: The inputs to the __init__ method are not rigidly defined. You can extend or modify them as needed to accommodate additional settings or configurations specific to your application.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representation: nn.Module, hidden_dim: int, n_actions: int, device: torch.device):\n",
    "        super(MyPolicy, self).__init__()\n",
    "        self.representation = representation  # Specify the representation.\n",
    "        self.feature_dim = self.representation.output_shapes['state'][0]  # Dimension of the representation's output.\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        ).to(device)  # The Q network.\n",
    "        self.target_q_net = deepcopy(self.q_net)  # Target Q network.\n",
    "\n",
    "    def forward(self, observation):\n",
    "        output_rep = self.representation(observation)  # Get the output of the representation module.\n",
    "        output = self.q_net(output_rep['state'])  # Get the output of the Q network.\n",
    "        argmax_action = output.argmax(dim=-1)  # Get greedy actions.\n",
    "        return output_rep, argmax_action, output\n",
    "\n",
    "    def target(self, observation):\n",
    "        outputs_target = self.representation(observation)  # Get the output of the representation module.\n",
    "        Q_target = self.target_q_net(outputs_target['state'])  # Get the output of the target Q network.\n",
    "        argmax_action = Q_target.argmax(dim=-1)  # Get greedy actions that output by target Q network.\n",
    "        return outputs_target, argmax_action.detach(), Q_target.detach()\n",
    "\n",
    "    def copy_target(self):  # Reset the parameters of target Q network as the Q network.\n",
    "        for ep, tp in zip(self.q_net.parameters(), self.target_q_net.parameters()):\n",
    "            tp.data.copy_(ep)\n",
    "            \n",
    "class MyLearner(Learner):\n",
    "    def __init__(self, config, policy, callback):\n",
    "        super(MyLearner, self).__init__(config, policy, callback)\n",
    "        # Build the optimizer.\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), self.config.learning_rate, eps=1e-5)\n",
    "        self.loss = nn.MSELoss()  # Build a loss function.\n",
    "        self.sync_frequency = config.sync_frequency  # The period to synchronize the target network.\n",
    "\n",
    "    def update(self, **samples):\n",
    "        info = {}\n",
    "        self.iterations += 1\n",
    "        '''Get a batch of training samples.'''\n",
    "        obs_batch = torch.as_tensor(samples['obs'], device=self.device)\n",
    "        act_batch = torch.as_tensor(samples['actions'], device=self.device)\n",
    "        next_batch = torch.as_tensor(samples['obs_next'], device=self.device)\n",
    "        rew_batch = torch.as_tensor(samples['rewards'], device=self.device)\n",
    "        ter_batch = torch.as_tensor(samples['terminals'], dtype=torch.float, device=self.device)\n",
    "\n",
    "        # Feedforward steps.\n",
    "        _, _, q_eval = self.policy(obs_batch)\n",
    "        _, _, q_next = self.policy.target(next_batch)\n",
    "        q_next_action = q_next.max(dim=-1).values\n",
    "        q_eval_action = q_eval.gather(-1, act_batch.long().unsqueeze(-1)).reshape(-1)\n",
    "        target_value = rew_batch + (1 - ter_batch) * self.gamma * q_next_action\n",
    "        loss = self.loss(q_eval_action, target_value.detach())\n",
    "\n",
    "        # Backward and optimizing steps.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Synchronize the target network\n",
    "        if self.iterations % self.sync_frequency == 0:\n",
    "            self.policy.copy_target()\n",
    "\n",
    "        # Set the variables you need to observe.\n",
    "        info.update({'loss': loss.item(),\n",
    "                     'iterations': self.iterations,\n",
    "                     'q_eval_action': q_eval_action.mean().item()})\n",
    "\n",
    "        return info\n",
    "    \n",
    "class MyAgent(OffPolicyAgent):\n",
    "    def __init__(self, config, envs):\n",
    "        super(MyAgent, self).__init__(config, envs)\n",
    "        self.policy = self._build_policy()  # Build the policy module.\n",
    "        self.memory = self._build_memory()  # Build the replay buffer.\n",
    "        REGISTRY_Learners['MyLearner'] = MyLearner  # Registry your pre-defined learner.\n",
    "        self.learner = self._build_learner(self.config, self.policy, None)  # Build the learner.\n",
    "\n",
    "    def _build_policy(self):\n",
    "        # First create the representation module.\n",
    "        representation = self._build_representation(\"Basic_MLP\", self.observation_space, self.config)\n",
    "        # Build your custom policy module.\n",
    "        policy = MyPolicy(representation, 64, self.action_space.n, self.config.device)\n",
    "        return policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42e262f4939423a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a config file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30b8137a75fa815c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "yaml_content = textwrap.dedent(\"\"\"\n",
    "    dl_toolbox: \"torch\"  # The deep learning toolbox. Choices: \"torch\", \"mindspore\", \"tensorlayer\"\n",
    "    project_name: \"XuanCe_New_Algorithm\"\n",
    "    logger: \"tensorboard\"  # Choices: tensorboard, wandb.\n",
    "    wandb_user_name: \"your_user_name\"\n",
    "    render: True\n",
    "    render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.\n",
    "    fps: 50\n",
    "    test_mode: True\n",
    "    device: \"cpu\"  # Choose an calculating device. PyTorch: \"cpu\", \"cuda:0\"; TensorFlow: \"cpu\"/\"CPU\", \"gpu\"/\"GPU\"; MindSpore: \"CPU\", \"GPU\", \"Ascend\", \"Davinci\".\n",
    "    distributed_training: False  # Whether to use multi-GPU for distributed training.\n",
    "    master_port: '12355'  # The master port for current experiment when use distributed training.\n",
    "    \n",
    "    agent: \"MyAgent\"\n",
    "    env_name: \"Classic Control\"\n",
    "    env_id: \"CartPole-v1\"\n",
    "    env_seed: 1\n",
    "    vectorize: \"DummyVecEnv\"\n",
    "    learner: \"MyLearner\"\n",
    "    policy: \"MyPolicy\"\n",
    "    representation: \"Basic_MLP\"\n",
    "    \n",
    "    representation_hidden_size: [64,]\n",
    "    q_hidden_size: [64,]\n",
    "    activation: 'relu'\n",
    "    \n",
    "    seed: 1\n",
    "    parallels: 10\n",
    "    buffer_size: 10000\n",
    "    batch_size: 256\n",
    "    learning_rate: 0.001\n",
    "    gamma: 0.99\n",
    "    \n",
    "    start_greedy: 0.5\n",
    "    end_greedy: 0.01\n",
    "    decay_step_greedy: 200000\n",
    "    sync_frequency: 50\n",
    "    training_frequency: 1\n",
    "    running_steps: 200000\n",
    "    start_training: 1000\n",
    "    \n",
    "    use_grad_clip: False  # gradient normalization\n",
    "    grad_clip_norm: 0.5\n",
    "    use_actions_mask: False\n",
    "    use_obsnorm: False\n",
    "    use_rewnorm: False\n",
    "    obsnorm_range: 5\n",
    "    rewnorm_range: 5\n",
    "    \n",
    "    test_steps: 10000\n",
    "    eval_interval: 20000\n",
    "    test_episode: 1\n",
    "    log_dir: \"./logs/my_agent/\"\n",
    "    model_dir: \"./models/my_agent/\"\n",
    "\"\"\")\n",
    "\n",
    "with open(\"new_rl.yaml\", \"w\") as f:\n",
    "    f.write(yaml_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a77985180c5084f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and run your algorithm\n",
    "\n",
    "Finally, we can create the agent and make environments to train the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea308b2165fff43a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = get_configs(file_dir=\"./new_rl.yaml\")  # Get the config settings from .yaml file.\n",
    "config = Namespace(**config)  # Convert the config from dict to argparse.\n",
    "envs = make_envs(config)  # Make vectorized environments.\n",
    "agent = MyAgent(config, envs)  # Instantiate your pre-build agent class.\n",
    "\n",
    "if not config.test_mode:  # Training mode.\n",
    "    agent.train(config.running_steps // envs.num_envs)  # Train your agent.\n",
    "    agent.save_model(\"final_train_model.pth\")  # After training, save the model.\n",
    "else:  # Testing mode.\n",
    "    config.parallels = 1  # Test on one environment.\n",
    "    env_fn = lambda: make_envs(config)  # The method to create testing environment.\n",
    "    agent.load_model(agent.model_dir_load)  # Load pre-trained model.\n",
    "    scores = agent.test(env_fn, config.test_episode)  # Test your agent.\n",
    "\n",
    "agent.finish()  # Finish the agent.\n",
    "envs.close()  # Close the environments."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4047c2c21aabd48b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full code\n",
    "\n",
    "The source code of this example can be visited at the following link:\n",
    "\n",
    "[https://github.com/agi-brain/xuance/blob/master/examples/new_algorithm/new_rl.py](https://github.com/agi-brain/xuance/blob/master/examples/new_algorithm/new_rl.py)\n",
    "\n",
    "Here we show the full code that you can run directly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d0b3fc1befe66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
